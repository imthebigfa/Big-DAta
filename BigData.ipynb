{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Step 1: Import Libraries and Initialize Spark"
      ],
      "metadata": {
        "id": "SLJV2COngmEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, trim, when, avg, count\n",
        "from pyspark.sql.types import StringType, DoubleType, IntegerType\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Telecom Churn Analysis\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "fNaN48V0g_8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Define Helper Functions"
      ],
      "metadata": {
        "id": "BZYtsN2ShKQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def null_prcnt_spark(df):\n",
        "    \"\"\"\n",
        "    Calculates the percentage of missing values in each column of a PySpark DataFrame.\n",
        "    \"\"\"\n",
        "    total_count = df.count()\n",
        "    null_percent = {\n",
        "        col_name: (df.filter(col(col_name).isNull()).count() / total_count) * 100\n",
        "        for col_name in df.columns\n",
        "    }\n",
        "    return null_percent"
      ],
      "metadata": {
        "id": "jer60VvzhnH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Read and Understand the Data"
      ],
      "metadata": {
        "id": "WJ_aJ2Hghtdu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace with the actual path of your dataset\n",
        "df = spark.read.csv('/content/drive/MyDrive/telecom_churn_data (1).csv', header=True, inferSchema=True)\n",
        "\n",
        "# Display schema and initial records\n",
        "df.printSchema()\n",
        "df.show(5)\n",
        "\n",
        "# Check missing values\n",
        "missing_vals = null_prcnt_spark(df)\n",
        "print(\"Missing Values:\\n\", missing_vals)"
      ],
      "metadata": {
        "id": "21lHl2NRh0C0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Data Cleaning and Manipulation\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Xk2XRAVQiA1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove whitespace from column names\n",
        "for c in df.columns:\n",
        "    if ' ' in c:  # Check if column name contains whitespace\n",
        "        df = df.withColumnRenamed(c, c.strip())\n",
        "\n",
        "# Drop duplicates\n",
        "df = df.dropDuplicates()\n",
        "\n",
        "# Rename columns\n",
        "df = df.withColumnRenamed('last_day_rch_amt_', 'last_day_rech_amt_')\n",
        "\n",
        "# Derive high-value customers\n",
        "high_value_threshold = 1000\n",
        "df = df.withColumnRenamed('total_rech_amt_6', 'monthly_revenue')\n",
        "\n",
        "# Create high_value column based on monthly_revenue\n",
        "df = df.withColumn('high_value', when(col('monthly_revenue') >= high_value_threshold, 1).otherwise(0))\n",
        "\n",
        "# Filter high-value customers\n",
        "df_high_value = df.filter(col('high_value') == 1)\n",
        "\n",
        "# Create churn column\n",
        "df = df.withColumn('churn', when(col('total_rech_amt_9') == 0, 1).otherwise(0))\n",
        "\n",
        "# Drop churn-phase columns (adjust pattern as needed)\n",
        "churn_phase_cols = [col_name for col_name in df.columns if 'month9' in col_name]\n",
        "df = df.drop(*churn_phase_cols)"
      ],
      "metadata": {
        "id": "CJrS_rHxiHDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Exploratory Data Analysis (EDA)"
      ],
      "metadata": {
        "id": "W5Uv_vQniRSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to Pandas for visualizations\n",
        "df_pandas = df.toPandas()\n",
        "\n",
        "# Select only numeric columns for correlation\n",
        "numeric_df = df_pandas.select_dtypes(include=['number'])\n",
        "\n",
        "# Plot monthly revenue distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df_pandas['monthly_revenue'], kde=True)\n",
        "plt.title('Monthly Revenue Distribution')\n",
        "plt.show()\n",
        "\n",
        "# Box plot for churn vs monthly revenue\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.kdeplot(data=df_pandas, x='monthly_revenue', hue='churn', fill=True, common_norm=False, alpha=0.5)\n",
        "plt.title('Density Plot: Monthly Revenue vs Churn')\n",
        "plt.xlabel('Monthly Revenue')\n",
        "plt.ylabel('Density')\n",
        "plt.show()\n",
        "\n",
        "# Correlation heatmap\n",
        "plt.figure(figsize=(14, 10))\n",
        "corr = numeric_df.corr()  # Calculate correlation on numeric columns only\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VxacwyjjiVJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Feature Combination and Modeling"
      ],
      "metadata": {
        "id": "vwXi9vCQiead"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine and average features for months 6 and 7\n",
        "df = df.withColumn('avg_revenue_6_7', (col('monthly_revenue') + col('total_rech_amt_7')) / 2)\n",
        "\n",
        "# Feature selection\n",
        "feature_columns = [\n",
        "    col_name\n",
        "    for col_name in df.columns\n",
        "    if col_name not in ['churn', 'high_value'] and df.schema[col_name].dataType != StringType()\n",
        "]\n",
        "\n",
        "# Impute missing values with 0 or mean\n",
        "for column in feature_columns:\n",
        "    df = df.fillna(0, subset=[column])  # Option 1: Fill with 0 (imputation can be replaced with mean if needed)\n",
        "\n",
        "# VectorAssembler to combine features into a single vector column\n",
        "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\", handleInvalid=\"skip\")\n",
        "df = assembler.transform(df)\n",
        "\n",
        "# Split data into training and testing sets (70/30)\n",
        "train, test = df.randomSplit([0.7, 0.3], seed=42)\n",
        "\n",
        "# Logistic Regression model\n",
        "lr = LogisticRegression(labelCol=\"churn\", featuresCol=\"features\", maxIter=100)\n",
        "model = lr.fit(train)\n",
        "\n",
        "# Predictions\n",
        "predictions = model.transform(test)\n",
        "\n",
        "# Evaluation: Accuracy\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"churn\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
        "auc = evaluator.evaluate(predictions)\n",
        "print(f\"Area Under ROC: {auc}\")\n",
        "\n",
        "# Optional: Classification report (convert to Pandas for sklearn classification report)\n",
        "predictions_pandas = predictions.select('churn', 'prediction').toPandas()\n",
        "print(classification_report(predictions_pandas['churn'], predictions_pandas['prediction']))\n",
        "\n",
        "# Optional: Drop the 'features' column after model evaluation if needed\n",
        "df = df.drop('features')\n"
      ],
      "metadata": {
        "id": "XGPmh9HRik0S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}